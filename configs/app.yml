embedding_service:
  url: "http://localhost:8001/embed"   # Endpoint for the external embedding API

qdrant:
  url: "http://localhost:6333"         # Qdrant vector database URL
  collection: "documents"              # Collection where all vectors are stored

chunking:
  max_chars: 800                       # Max characters per chunk before embedding

ingestion:
  batch_size: 500                      # Number of chunks to upload to Qdrant per batch

embedding:
  dimension: 768                       # Size of dense embedding vectors (E5 output size)

redis:
  url: "redis://localhost:6379"        # Redis instance for conversation memory

llm:
  provider: "ollama"                   # LLM backend: "ollama" or "vllm"
  model: "llama3.1:8b"                 # Model name to load from the provider
  base_url: "http://localhost:11434"   # LLM server endpoint
  temperature: 0.1                     # Controls creativity vs. determinism

agent_cache:
  maxsize: 100        # maximum number of cached agents to keep in memory
  ttl_seconds: 3600   # how long each cached agent stays alive (1 hour)

reranker:
  model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"

retrieval:
  hybrid_limit: 20        # how many candidates to fetch from Qdrant + ES
  top_k: 5                # how many final chunks to return
  reranker_threshold: 0.7 # minimum crossâ€‘encoder score
